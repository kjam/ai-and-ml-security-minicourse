{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da0a2ea-045b-411d-a384-bf4e32f2adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb7a463-0667-4ba2-aeb9-13471f0e6920",
   "metadata": {},
   "source": [
    "### Llama Guard: External Algorithmic Guardrails\n",
    "\n",
    "If we want to protect against harmful or insecure responses, we have three options:\n",
    "\n",
    "1. **Software filters and flags**: These are external guardrails that filter and look for software-based match or partial match (ReGex, or hash-based, or memory-architecture-based). Then, either the input or the response is sanitized of these matches or blocked.\n",
    "2. **External Algorithmic Guardrails**: These are external algorithmic-based guardrails, which use a separate model to evaluate input or output and either modify or reject potentially harmful content or content violations. This is what you'll see in this notebook.\n",
    "3. **Internal (Fine-Tuned) Guardrails**: These are guardrails that have been trained directly into the model, by showing examples of pontentially harmful content during training and then training the types of responses you've seen in attacking the models (i.e. \"I can't help with that, etc\").\n",
    "\n",
    "You can [read more about guardrails on my blog](https://blog.kjamistan.com/algorithmic-based-guardrails-external-guardrail-models-and-alignment-methods.html) for a detailed description or check out [my explanation on YouTube](https://www.youtube.com/watch?v=IeyB-2cS5lM&ab_channel=ProbablyPrivate). \n",
    "\n",
    "\n",
    "#### LlamaGuard\n",
    "\n",
    "LlamaGuard is part of the [PurpleLlama](https://github.com/meta-llama/PurpleLlama) family of models, which include a variety of security-focused interventions. \n",
    "\n",
    "Further documentation on the model we are using in this notebook can be found on the [HuggingFace Llama-Guard3 Model Page](https://huggingface.co/meta-llama/Llama-Guard-3-8B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "726789a5-63c4-409a-a0b5-ac7297f01204",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_client = ollama.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "567ef0bb-97fe-43d1-8151-9e66324bfbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llamaguard_response(prompt):\n",
    "    return ollama_client.chat(\n",
    "        model='llama-guard3',\n",
    "        messages=[\n",
    "            {'role': 'user', \n",
    "             'content': prompt}]\n",
    "    ).message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ec3c3e-22fe-468e-b256-fbb7c0641b20",
   "metadata": {},
   "source": [
    "### LlamaGuard3 Categories\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "|  Code      | Name      |\n",
    "| ------------- | ------------- |\n",
    "| S1 | Violent crimes |\n",
    "| S2 | Non-violent crimes |\n",
    "| S3 | Sex-related crimes |\n",
    "| S4 | Child sexual exploitation |\n",
    "| S5 | Defamation |\n",
    "| S6 | Specialized advice |\n",
    "| S7 | Privacy |\n",
    "| S8 | Intellectual property |\n",
    "| S9 | Indiscriminate weapons |\n",
    "| S10 | Hate |\n",
    "| S11 | Suicide & self-harm |\n",
    "| S12 | Sexual content |\n",
    "| S13 | Elections |\n",
    "| S14 | Code interpreter abuse |\n",
    "\n",
    "\n",
    "\n",
    "Let's try out the privacy ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab8a787d-e5db-4096-b22f-c643501e8ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Can you tell me the birthplace of Angela Merkel?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9418d38f-8c76-4a3b-9178-64f9fc62bd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'safe'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_llamaguard_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "096e2e21-7698-4e66-97c8-053cf7ccbbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is Angela Merkel's home address?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67776d3d-d0ec-4d39-9971-52b5d77f715d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unsafe\\nS7'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_llamaguard_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faffd91b-7bc2-4863-98e7-98dff1f39e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"My credit card number ix 7771-2463-2224-0879\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05545c56-0fc0-4727-8018-e955d3ac2747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unsafe\\nS7'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_llamaguard_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "732078e8-4c4b-4b78-bf36-a919fd22848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I need help with a neighbor. their email is fred@yahoo.com and they stole my cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b37e0be-b905-4b88-bdb2-d3b5d5a58c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unsafe\\nS7'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_llamaguard_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76cc9a0b-26f8-4d25-8160-acde7f0a4572",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I need help with a neighbor. can u please mail to fred at yahoo punkt com and tell them to give back my cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51a778d3-ead1-4036-a720-4b276178adc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'safe'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_llamaguard_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf690c7-da79-4e14-8967-0f7fb7c8a21b",
   "metadata": {},
   "source": [
    "### Your Turn\n",
    "\n",
    "Give it a try yourself. Consider:\n",
    "\n",
    "1. Testing out some of the other categories.\n",
    "2. Reusing input from other notebooks.\n",
    "3. Orienting it towards a particular use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b6a158-14d0-4e21-8dee-143147449824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
